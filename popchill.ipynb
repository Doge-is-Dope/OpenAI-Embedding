{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import tiktoken  # for counting tokens\n",
    "import pandas as pd  # for storing text and embeddings data\n",
    "import string  # for removing punctuation\n",
    "import unicodedata  # for normalizing text\n",
    "import urllib.request  # for downloading html\n",
    "from bs4 import BeautifulSoup  # for parsing html\n",
    "from collections import deque\n",
    "from html.parser import HTMLParser\n",
    "from openai import OpenAI  # for calling the OpenAI API\n",
    "from scipy import spatial  # for calculating vector similarities for search\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    wait_random_exponential,\n",
    "    stop_after_attempt,\n",
    ")  # for retrying API calls\n",
    "from urllib.parse import urlparse\n",
    "from utils.embeddings_utils import cosine_similarity\n",
    "\n",
    "# models\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "GPT_MODEL = \"gpt-4o\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "domain = \"www.popchill.com\"\n",
    "full_url = \"https://www.popchill.com\"\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawl Web Pages\n",
    "\n",
    "Given a list of URLs, crawl each page and return the content of the page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex pattern to match a URL\n",
    "HTTP_URL_PATTERN = r\"^http[s]*://.+\"\n",
    "\n",
    "\n",
    "# Create a class to parse the HTML and get the hyperlinks\n",
    "class HyperlinkParser(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Create a list to store the hyperlinks\n",
    "        self.hyperlinks = []\n",
    "\n",
    "    # Override the HTMLParser's handle_starttag method to get the hyperlinks\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        attrs = dict(attrs)\n",
    "\n",
    "        # If the tag is an anchor tag and it has an href attribute, add the href attribute to the list of hyperlinks\n",
    "        if tag == \"a\" and \"href\" in attrs:\n",
    "            self.hyperlinks.append(attrs[\"href\"])\n",
    "\n",
    "\n",
    "# Function to get the hyperlinks from a URL\n",
    "def get_hyperlinks(url):\n",
    "\n",
    "    user_agent = \"Mozilla/5.0\"\n",
    "    headers = {\"User-Agent\": user_agent}\n",
    "\n",
    "    # Try to open the URL and read the HTML\n",
    "    try:\n",
    "        request = urllib.request.Request(url=url, headers=headers)\n",
    "        # Open the URL and read the HTML\n",
    "        with urllib.request.urlopen(request) as response:\n",
    "\n",
    "            # If the response is not HTML, return an empty list\n",
    "            if not response.info().get(\"Content-Type\").startswith(\"text/html\"):\n",
    "                return []\n",
    "\n",
    "            # Decode the HTML\n",
    "            html = response.read().decode(\"utf-8\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return []\n",
    "\n",
    "    # Create the HTML Parser and then Parse the HTML to get hyperlinks\n",
    "    parser = HyperlinkParser()\n",
    "    parser.feed(html)\n",
    "\n",
    "    return parser.hyperlinks\n",
    "\n",
    "\n",
    "# Function to get the hyperlinks from a URL that are within the same domain\n",
    "def get_domain_hyperlinks(local_domain, url):\n",
    "    clean_links = []\n",
    "    for link in set(get_hyperlinks(url)):\n",
    "        clean_link = None\n",
    "\n",
    "        # If the link is a URL, check if it is within the same domain\n",
    "        if re.search(HTTP_URL_PATTERN, link):\n",
    "            # Parse the URL and check if the domain is the same\n",
    "            url_obj = urlparse(link)\n",
    "            if url_obj.netloc == local_domain:\n",
    "                clean_link = link\n",
    "\n",
    "        # If the link is not a URL, check if it is a relative link\n",
    "        else:\n",
    "            if link.startswith(\"/\"):\n",
    "                link = link[1:]\n",
    "            elif link.startswith(\"#\") or link.startswith(\"mailto:\"):\n",
    "                continue\n",
    "            clean_link = \"https://\" + local_domain + \"/\" + link\n",
    "\n",
    "        if clean_link is not None:\n",
    "            if clean_link.endswith(\"/\"):\n",
    "                clean_link = clean_link[:-1]\n",
    "            clean_links.append(clean_link)\n",
    "\n",
    "    # Return the list of hyperlinks that are within the same domain\n",
    "    return list(set(clean_links))\n",
    "\n",
    "\n",
    "def get_content(soup):\n",
    "    try:\n",
    "        name = soup.find(\"h2\", class_=\"chakra-text\").text\n",
    "        description = soup.find(\"div\", class_=\"css-icjp7f\").text\n",
    "        data = soup.find(\"div\", class_=\"css-4cxybv\")\n",
    "        tags = [\n",
    "            span.get_text().lower()\n",
    "            for span in data.find_all(\"span\", class_=\"css-1ny2kle\")\n",
    "        ]\n",
    "        return name, description, tags\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def crawl(url):\n",
    "    # Parse the URL and get the domain\n",
    "    local_domain = urlparse(url).netloc\n",
    "\n",
    "    # Create a queue to store the URLs to crawl\n",
    "    queue = deque([url])\n",
    "\n",
    "    # Create a set to store the URLs that have already been seen (no duplicates)\n",
    "    seen = set([url])\n",
    "\n",
    "    # Create a directory to store the text files\n",
    "    if not os.path.exists(\"text/\"):\n",
    "        os.mkdir(\"text/\")\n",
    "\n",
    "    if not os.path.exists(\"text/\" + local_domain + \"/\"):\n",
    "        os.mkdir(\"text/\" + local_domain + \"/\")\n",
    "\n",
    "    # Create a directory to store the csv files\n",
    "    if not os.path.exists(\"processed\"):\n",
    "        os.mkdir(\"processed\")\n",
    "\n",
    "    # While the queue is not empty, continue crawling\n",
    "    while queue:\n",
    "\n",
    "        # Get the next URL from the queue\n",
    "        url = queue.pop()\n",
    "        print(url)  # for debugging and to see the progress\n",
    "\n",
    "        # Save text from the url to a <url>.txt file\n",
    "        with open(\n",
    "            \"text/\" + local_domain + \"/\" + url[8:].replace(\"/\", \"_\") + \".txt\", \"w\"\n",
    "        ) as f:\n",
    "\n",
    "            # Get the text from the URL using BeautifulSoup\n",
    "            soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n",
    "\n",
    "            # Get the content from the URL\n",
    "            text = soup.get_text()\n",
    "\n",
    "            # If the crawler gets to a page that requires JavaScript, it will stop the crawl\n",
    "            if \"You need to enable JavaScript to run this app.\" in text:\n",
    "                print(\n",
    "                    \"Unable to parse page \" + url + \" due to JavaScript being required\"\n",
    "                )\n",
    "\n",
    "            # Write the data to the file\n",
    "            f.write(text)\n",
    "\n",
    "        # Get the hyperlinks from the URL and add them to the queue\n",
    "        for link in get_domain_hyperlinks(local_domain, url):\n",
    "            if link not in seen:\n",
    "                queue.append(link)\n",
    "                seen.add(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl(full_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_newlines(serie):\n",
    "    \"\"\"Remove newlines from a pandas series for better processing.\"\"\"\n",
    "    serie = serie.str.replace(\"\\n\", \" \")\n",
    "    serie = serie.str.replace(\"\\\\n\", \" \")\n",
    "    serie = serie.str.replace(\"  \", \" \")\n",
    "    serie = serie.str.replace(\"  \", \" \")\n",
    "    return serie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store the data records\n",
    "data = []\n",
    "\n",
    "# Get all the text files in the text directory\n",
    "for file in os.listdir(\"text/\" + domain + \"/\"):\n",
    "\n",
    "    # Open the file and read the text\n",
    "    with open(\"text/\" + domain + \"/\" + file, \"r\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "        # Omit the first 11 lines and the last 4 lines, then replace -, _, and #update with spaces.\n",
    "        data.append(\n",
    "            (\n",
    "                file[len(domain) + 7 : -4]\n",
    "                .replace(\"-\", \" \")\n",
    "                .replace(\"_\", \" \")\n",
    "                .replace(\"#update\", \"\"),\n",
    "                text,\n",
    "            )\n",
    "        )\n",
    "# Create a dataframe from the list of texts\n",
    "df = pd.DataFrame(data, columns=[\"fname\", \"text\"])\n",
    "df[\"text\"] = df.fname + \". \" + remove_newlines(df.text)\n",
    "\n",
    "if not os.path.exists(\"processed/\" + domain + \"/\"):\n",
    "    os.mkdir(\"processed/\" + domain + \"/\")\n",
    "df.to_csv(f\"processed/{domain}/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retry up to 6 times with exponential backoff, starting at 1 second and maxing out at 20 seconds delay\n",
    "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
    "def get_embedding(text: str, model=EMBEDDING_MODEL):\n",
    "    # replace newlines, which can negatively affect performance.\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "\n",
    "    response = client.embeddings.create(input=[text], model=model)\n",
    "\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>text</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>product 24011057169632</td>\n",
       "      <td>product 24011057169632. PopChill x 滙豐信用卡持卡人最紅優...</td>\n",
       "      <td>[0.024357043206691742, 0.015573929063975811, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>event 5stars?t=pc menu</td>\n",
       "      <td>event 5stars?t=pc menu. 五星好店｜PopChill 拍拍圈搜尋商品或...</td>\n",
       "      <td>[0.023113323375582695, 0.010650375857949257, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>brand series 83</td>\n",
       "      <td>brand series 83. 二手ChloéDrew包包優惠！2024 05月人氣推薦好...</td>\n",
       "      <td>[0.05937468633055687, -0.006741405464708805, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>user nicebuyintw</td>\n",
       "      <td>user nicebuyintw. 拉堤二手名牌(@nicebuyintw) 的衣櫥｜Pop...</td>\n",
       "      <td>[0.016361327841877937, -0.020826730877161026, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>search?q=%E6%98%A5%E5%A4%A9</td>\n",
       "      <td>search?q=%E6%98%A5%E5%A4%A9. 春天 價格優惠！2024 05月人...</td>\n",
       "      <td>[0.03858361393213272, 0.011575084179639816, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         fname  \\\n",
       "0       product 24011057169632   \n",
       "1       event 5stars?t=pc menu   \n",
       "2              brand series 83   \n",
       "3             user nicebuyintw   \n",
       "4  search?q=%E6%98%A5%E5%A4%A9   \n",
       "\n",
       "                                                text  \\\n",
       "0  product 24011057169632. PopChill x 滙豐信用卡持卡人最紅優...   \n",
       "1  event 5stars?t=pc menu. 五星好店｜PopChill 拍拍圈搜尋商品或...   \n",
       "2  brand series 83. 二手ChloéDrew包包優惠！2024 05月人氣推薦好...   \n",
       "3  user nicebuyintw. 拉堤二手名牌(@nicebuyintw) 的衣櫥｜Pop...   \n",
       "4  search?q=%E6%98%A5%E5%A4%A9. 春天 價格優惠！2024 05月人...   \n",
       "\n",
       "                                           embedding  \n",
       "0  [0.024357043206691742, 0.015573929063975811, -...  \n",
       "1  [0.023113323375582695, 0.010650375857949257, -...  \n",
       "2  [0.05937468633055687, -0.006741405464708805, 0...  \n",
       "3  [0.016361327841877937, -0.020826730877161026, ...  \n",
       "4  [0.03858361393213272, 0.011575084179639816, 0....  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"embedding\"] = df[\"text\"].apply(lambda x: get_embedding(x))\n",
    "df.to_csv(f\"processed/{domain}/embeddings.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_relatedness(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    relatedness_fn=lambda x, y: cosine_similarity(x, y),\n",
    "    top_n: int = 100,\n",
    "):\n",
    "    # Get the embedding for the query\n",
    "    query_embedding = get_embedding(query)\n",
    "\n",
    "    strings_and_relatednesses = [\n",
    "        (row[\"text\"], relatedness_fn(query_embedding, row[\"embedding\"]))\n",
    "        for i, row in df.iterrows()\n",
    "    ]\n",
    "\n",
    "    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
    "    strings, relatednesses = zip(*strings_and_relatednesses)\n",
    "    return strings[:top_n], relatednesses[:top_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens(text: str, model: str = GPT_MODEL) -> int:\n",
    "    \"\"\"Return the number of tokens in a string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "\n",
    "def query_message(query: str, df: pd.DataFrame, model: str, token_budget: int) -> str:\n",
    "    \"\"\"Return a message for GPT, with relevant source texts pulled from a dataframe.\"\"\"\n",
    "    strings, relatedness = search_relatedness(query, df)\n",
    "    introduction = '你是 PopChill 的客服，你根據資料回答 \"PopChill\" 的問題。'\n",
    "    question = f\"\\n\\nQuestion: {query}\"\n",
    "    message = introduction\n",
    "    for string in strings:\n",
    "        data = f'\\n\\nData:\\n\"\"\"\\n{string}\\n\"\"\"'\n",
    "        if num_tokens(message + data + question, model=model) > token_budget:\n",
    "            break\n",
    "        else:\n",
    "            message += data\n",
    "    return message + question\n",
    "\n",
    "\n",
    "def ask(\n",
    "    query: str,\n",
    "    df: pd.DataFrame = df,\n",
    "    model: str = GPT_MODEL,\n",
    "    token_budget: int = 4096 - 500,\n",
    "    print_message: bool = False,\n",
    ") -> str:\n",
    "    \"\"\"Answers a query using GPT and a dataframe of relevant texts and embeddings.\"\"\"\n",
    "    message = query_message(query, df, model=model, token_budget=token_budget)\n",
    "    if print_message:\n",
    "        print(message)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"你回答任何關於 PopChill 的問題\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": message},\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model, messages=messages, temperature=0\n",
    "    )\n",
    "    response_message = response.choices[0].message.content\n",
    "    return response_message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PopChill 是一個提供中間驗證服務的 C2C 二手精品交易平台。它專注於二手名牌商品的買賣，並通過 Entrupy 及 LegitApp 等美國第三方服務對商品進行正品鑑定。賣家將商品寄至 PopChill 進行鑑定，確認為正品後再出貨給買家。PopChill 旨在提供安心購物的體驗，並且與平台上銷售的品牌方無任何關聯或從屬關係。PopChill 通過經濟部中小企業處小型企業創新研發計畫 (SBIR) 補助。'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask(\"PopChill 是什麼？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'根據提供的資料，最受歡迎的商品是 \"product 23092840179685\"。這款商品已經售出，並且提供了多種無卡分期付款選項，吸引了許多新客和舊客。'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask(\"最多人買的商品是什麼？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'在 PopChill 台灣站上，您可以找到以下類型的商品：\\n\\n1. **側 / 肩背包**\\n2. **皮夾**\\n3. **手提包**\\n4. **後背包**\\n\\n這些商品涵蓋了多個知名品牌，包括但不限於：\\n- Coach\\n- Gucci\\n- Louis Vuitton\\n- Tory Burch\\n- Chanel\\n- BURBERRY\\n- Prada\\n\\n這些商品主要來自於多個賣家，包括拉堤二手名牌、台中米蘭站、香榭國際精品、流行工廠名牌二手店等。所有商品都經過嚴格的驗證，確保正品。'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask(\"台灣站都賣哪些商品？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'根據提供的資料，PopChill 並沒有公開具體用戶的銷售數據或排名。因此，我無法確定哪一個用戶賣得最好。如果您有其他問題或需要進一步的幫助，請隨時告訴我。您也可以通過 PopChill 的客服信箱 support@popchill.com 聯絡我們的客服團隊。'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask(\"哪一個用戶賣得最好？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'根據提供的資料，PopChill 並未提及有 Line 官方帳號。如果您有其他問題或需要更多幫助，請隨時聯絡我們的客服信箱：support@popchill.com。'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask(\"有 Line 官方帳號嗎？\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
